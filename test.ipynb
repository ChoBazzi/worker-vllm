{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "07230369",
      "metadata": {},
      "source": [
        "# worker-vllm Colab Smoke Test\n",
        "\n",
        "This notebook installs explicit package versions and runs quick tests for `transformers` and `vllm`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "27202bce",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "Platform: Linux-6.6.105+-x86_64-with-glibc2.35\n",
            "COLAB_GPU: 1\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import platform\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "print('Python:', sys.version)\n",
        "print('Platform:', platform.platform())\n",
        "print('COLAB_GPU:', os.environ.get('COLAB_GPU', 'N/A'))\n",
        "\n",
        "try:\n",
        "    subprocess.run(['nvidia-smi'], check=False)\n",
        "except FileNotFoundError:\n",
        "    print('nvidia-smi not found. In Colab, set Runtime -> GPU first.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "881843d9",
      "metadata": {},
      "source": [
        "## 1) Repo Path Setup\n",
        "\n",
        "- If this notebook is already opened inside the repo, it uses current directory.\n",
        "- Otherwise it clones `runpod-workers/worker-vllm` to `/content/worker-vllm`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "42dff4c3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using repo: /content/worker-vllm\n",
            "requirements: True\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "REPO_URL = 'https://github.com/runpod-workers/worker-vllm.git'\n",
        "REPO_DIR = Path('/content/worker-vllm')\n",
        "\n",
        "if (Path.cwd() / 'Dockerfile').exists() and (Path.cwd() / 'builder/requirements.txt').exists():\n",
        "    REPO_DIR = Path.cwd()\n",
        "elif not REPO_DIR.exists():\n",
        "    subprocess.run(['git', 'clone', REPO_URL, str(REPO_DIR)], check=True)\n",
        "\n",
        "os.chdir(REPO_DIR)\n",
        "print('Using repo:', REPO_DIR)\n",
        "print('requirements:', (REPO_DIR / 'builder/requirements.txt').exists())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fa54cd9",
      "metadata": {},
      "source": [
        "## 2) Install Dependencies\n",
        "\n",
        "This installs your requested package list directly, then installs `vllm==0.15.1`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3ceef127",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "$ /usr/bin/python3 -m pip install --upgrade pip\n",
            "$ /usr/bin/python3 -m pip install ray pandas pyarrow runpod>=1.8,<2.0 huggingface-hub packaging typing-extensions>=4.8.0 pydantic pydantic-settings hf-transfer transformers>=4.57.6 bitsandbytes>=0.45.0 kernels torch==2.6.0 autoawq\n",
            "$ /usr/bin/python3 -m pip install vllm==0.15.0\n",
            "Dependency install complete.\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "PACKAGES = [\n",
        "    'ray',\n",
        "    'pandas',\n",
        "    'pyarrow',\n",
        "    'runpod>=1.8,<2.0',\n",
        "    'huggingface-hub',\n",
        "    'packaging',\n",
        "    'typing-extensions>=4.8.0',\n",
        "    'pydantic',\n",
        "    'pydantic-settings',\n",
        "    'hf-transfer',\n",
        "    'transformers>=4.57.6',\n",
        "    'bitsandbytes>=0.45.0',\n",
        "    'kernels',\n",
        "    'torch==2.6.0',\n",
        "    \"autoawq\"\n",
        "\n",
        "]\n",
        "\n",
        "commands = [\n",
        "    [sys.executable, '-m', 'pip', 'install', '--upgrade', 'pip'],\n",
        "    [sys.executable, '-m', 'pip', 'install', *PACKAGES],\n",
        "    [sys.executable, '-m', 'pip', 'install', 'vllm==0.15.0'],\n",
        "]\n",
        "\n",
        "for cmd in commands:\n",
        "    print('$', ' '.join(cmd))\n",
        "    subprocess.run(cmd, check=True)\n",
        "\n",
        "print('Dependency install complete.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "217cae8d",
      "metadata": {},
      "source": [
        "If imports fail right after install, restart runtime once and rerun from the next cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35c0c501",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch: 2.9.1+cu128\n",
            "torch CUDA: 12.8\n",
            "cuda available: True\n",
            "transformers: 4.57.6\n",
            "vllm: 0.15.0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import transformers\n",
        "import vllm\n",
        "\n",
        "print('torch:', torch.__version__)\n",
        "print('torch CUDA:', torch.version.cuda)\n",
        "print('cuda available:', torch.cuda.is_available())\n",
        "print('transformers:', transformers.__version__)\n",
        "print('vllm:', vllm.__version__)\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "    raise RuntimeError('CUDA GPU not detected. Use a GPU runtime in Colab.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac29ddd9",
      "metadata": {},
      "source": [
        "## 3) Optional: worker-vllm Import Check\n",
        "\n",
        "Some Colab `vllm` builds may not expose `vllm.entrypoints.openai.protocol`.\n",
        "In that case, this section is skipped and you can continue with inference smoke tests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9ce6b564",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vllm: 0.15.0\n",
            "has vllm.entrypoints.openai.protocol: False\n",
            "Skip: worker-vllm import check (module path not available in this build).\n"
          ]
        }
      ],
      "source": [
        "import importlib.util\n",
        "import os\n",
        "import vllm\n",
        "\n",
        "os.environ['MODEL_NAME'] = os.environ.get('MODEL_NAME', 'Qwen/Qwen2.5-7B-Instruct-AWQ')\n",
        "os.environ['TOKENIZER_NAME'] = os.environ.get('TOKENIZER_NAME', os.environ['MODEL_NAME'])\n",
        "os.environ['MAX_MODEL_LEN'] = os.environ.get('MAX_MODEL_LEN', '2048')\n",
        "os.environ['GPU_MEMORY_UTILIZATION'] = os.environ.get('GPU_MEMORY_UTILIZATION', '0.90')\n",
        "\n",
        "print('vllm:', vllm.__version__)\n",
        "protocol_spec = importlib.util.find_spec('vllm.entrypoints.openai.protocol')\n",
        "print('has vllm.entrypoints.openai.protocol:', bool(protocol_spec))\n",
        "\n",
        "if protocol_spec is None:\n",
        "    print('Skip: worker-vllm import check (module path not available in this build).')\n",
        "else:\n",
        "    try:\n",
        "        from src.engine_args import get_engine_args\n",
        "        engine_args = get_engine_args()\n",
        "        print(engine_args)\n",
        "    except Exception as e:\n",
        "        print('worker-vllm import check failed:', repr(e))\n",
        "        print('Continue with sections 4 and 5 for transformers/vllm smoke tests.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c982da71",
      "metadata": {},
      "source": [
        "## 4) transformers Smoke Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49d549a8",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "We suggest you to set `dtype=torch.float16` for better efficiency on CUDA/XPU with AWQ.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d5c3bd5c943d4595bb7ad79d5c096773",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "264128b741a74a769c84f1c0d89b2587",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "87550a01c709428c93752cdeec58f984",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/1.24G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ca75008565be454997e4b551f40bedd8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/4.85G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "TypeError",
          "evalue": "Qwen3ForCausalLM.__init__() got an unexpected keyword argument 'quantization'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-171260375.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEST_MODEL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mTEST_MODEL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m                 \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    605\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mold_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4969\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mContextManagers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_init_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4970\u001b[0m             \u001b[0;31m# Let's make sure we don't run the init function of buffer modules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4971\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4973\u001b[0m         \u001b[0;31m# Make sure to tie the weights correctly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Qwen3ForCausalLM.__init__() got an unexpected keyword argument 'quantization'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "TEST_MODEL = os.environ.get('TEST_MODEL', os.environ.get('MODEL_NAME', 'Qwen/Qwen3-8B-AWQ'))\n",
        "PROMPT = 'Write one short sentence about why low-latency inference matters.'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(TEST_MODEL, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    TEST_MODEL,\n",
        "    torch_dtype='auto',\n",
        "    device_map='auto',\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "inputs = tokenizer(PROMPT, return_tensors='pt').to(device)\n",
        "with torch.no_grad():\n",
        "    output_ids = model.generate(**inputs, max_new_tokens=40, do_sample=False)\n",
        "\n",
        "transformers_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "print('Model:', TEST_MODEL)\n",
        "print(transformers_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1598cd4",
      "metadata": {},
      "source": [
        "## 5) vLLM Smoke Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5d9899e6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 02-07 17:09:37 [utils.py:261] non-default args: {'tokenizer': 'Qwen/Qwen3-8B-AWQ', 'trust_remote_code': True, 'disable_log_stats': True, 'model': 'Qwen/Qwen3-8B-AWQ'}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
            "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 02-07 17:09:56 [model.py:541] Resolved architecture: Qwen3ForCausalLM\n",
            "INFO 02-07 17:09:56 [model.py:1561] Using max model len 40960\n",
            "INFO 02-07 17:09:56 [awq_marlin.py:162] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
            "INFO 02-07 17:09:56 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "633a6812d0294dfc8699fe4e27b1b591",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Parse safetensors files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 02-07 17:09:59 [vllm.py:624] Asynchronous scheduling is enabled.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b96ac59731c746b3a2b26c4bdff67209",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING 02-07 17:10:01 [system_utils.py:140] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m코드를 실행할 수 없습니다. 세션이 삭제되었습니다. 커널을 다시 시작해 보세요."
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from vllm import LLM, SamplingParams\n",
        "\n",
        "# Free VRAM before creating vLLM engine.\n",
        "if 'model' in globals():\n",
        "    del model\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "sampling_params = SamplingParams(temperature=0.0, max_tokens=40)\n",
        "llm = LLM(\n",
        "    model=TEST_MODEL,\n",
        "    tokenizer=TEST_MODEL,\n",
        "    trust_remote_code=True,\n",
        "    gpu_memory_utilization=float(os.environ.get('GPU_MEMORY_UTILIZATION', '0.90')),\n",
        ")\n",
        "\n",
        "outputs = llm.generate([PROMPT], sampling_params)\n",
        "vllm_output = outputs[0].outputs[0].text.strip()\n",
        "print(vllm_output)\n",
        "\n",
        "if not vllm_output:\n",
        "    raise RuntimeError('vLLM output is empty.')\n",
        "\n",
        "print('vLLM smoke test passed.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b6cbabe",
      "metadata": {},
      "source": [
        "Done. If you want to test a different model, set `MODEL_NAME`/`TEST_MODEL` and rerun sections 4 and 5."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
